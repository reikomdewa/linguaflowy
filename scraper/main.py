import json
import os
import requests
import time
import random
import re

# Reliable Piped Instances
INSTANCES = [
    "https://pipedapi.kavin.rocks",
    "https://api.piped.privacy.com.de",
    "https://piped-api.lunar.icu",
    "https://api.piped.uot.OI"
]

LANGUAGES = {
    'es': 'Spanish comprehensible input',
    'fr': 'French comprehensible input',
    'de': 'German comprehensible input',
    'it': 'Italian comprehensible input',
    'pt': 'Portuguese comprehensible input',
    'ja': 'Japanese comprehensible input',
    'en': 'English stories'
}

def get_working_instance():
    """Finds a Piped instance that is online."""
    for url in INSTANCES:
        try:
            print(f"Testing connection to {url}...")
            r = requests.get(f"{url}/trending?region=US", timeout=5)
            if r.status_code == 200:
                print(f"  ✅ Connected to {url}")
                return url
        except:
            continue
    print("  ❌ No instances available.")
    return None

def clean_vtt_text(vtt_content):
    """Cleans WebVTT subtitle text."""
    lines = vtt_content.splitlines()
    clean_lines = []
    seen = set()
    # Regex for timestamps: 00:00:00.000 --> 00:00:00.000
    timestamp_pattern = re.compile(r'\d{2}:\d{2}:\d{2}\.\d{3}\s-->\s\d{2}:\d{2}:\d{2}\.\d{3}')

    for line in lines:
        line = line.strip()
        if (not line or line == 'WEBVTT' or line.startswith('Kind:') or 
            line.startswith('Language:') or timestamp_pattern.search(line) or line.isdigit()):
            continue
        # Remove HTML tags
        line = re.sub(r'<[^>]+>', '', line)
        
        if line not in seen:
            clean_lines.append(line)
            seen.add(line)
    return " ".join(clean_lines)

def get_transcript(instance, video_id, lang_code):
    """Fetches subtitles from Piped API."""
    try:
        url = f"{instance}/streams/{video_id}"
        r = requests.get(url, timeout=10)
        if r.status_code != 200: return None
        
        data = r.json()
        subtitles = data.get('subtitles', [])
        
        # Find matching subtitle
        # 1. Exact match
        sub = next((s for s in subtitles if s['code'] == lang_code), None)
        # 2. Auto-generated match
        if not sub:
            sub = next((s for s in subtitles if s['code'] == lang_code and s.get('autoGenerated')), None)
            
        if not sub: return None
        
        # Download VTT
        vtt_r = requests.get(sub['url'], timeout=10)
        if vtt_r.status_code != 200: return None
        
        return clean_vtt_text(vtt_r.text)
        
    except Exception as e:
        print(f"    ! Transcript error: {e}")
        return None

def analyze_difficulty(text):
    words = text.split()
    if not words: return 'intermediate'
    avg_len = sum(len(w) for w in words) / len(words)
    if avg_len < 4.5: return 'beginner'
    if avg_len > 6.0: return 'advanced'
    return 'intermediate'

def scrape_language(instance, lang_code, query):
    print(f"\n--- Searching: {query} ({lang_code}) ---")
    lessons = []
    
    try:
        # Piped Search API
        search_url = f"{instance}/search?q={query}&filter=videos"
        r = requests.get(search_url, timeout=10)
        
        if r.status_code != 200:
            print("  ! Search failed")
            return []
            
        results = r.json().get('items', [])
        
        for video in results:
            if len(lessons) >= 6: break
            
            # Piped returns URL like "/watch?v=ID"
            video_id = video['url'].replace('/watch?v=', '')
            title = video['title']
            thumbnail = video['thumbnail']
            
            print(f"  > Checking: {title}")
            
            # Small delay to be polite
            time.sleep(random.uniform(0.5, 1.5))
            
            content = get_transcript(instance, video_id, lang_code)
            
            if content and len(content) > 100:
                print(f"    + SUCCESS: {len(content)} chars")
                lessons.append({
                    "id": f"yt_{video_id}",
                    "userId": "system",
                    "title": title,
                    "language": lang_code,
                    "content": content,
                    "sentences": [], 
                    "createdAt": "2024-01-01T00:00:00.000Z",
                    "imageUrl": thumbnail,
                    "type": "video",
                    "difficulty": analyze_difficulty(content),
                    "videoUrl": f"https://youtube.com/watch?v={video_id}",
                    "isFavorite": False
                })
            else:
                print("    - No subtitles found")
                
    except Exception as e:
        print(f"  ! Error scraping {lang_code}: {e}")
        
    return lessons

def main():
    if not os.path.exists('data'):
        os.makedirs('data')
        
    # Get a working instance
    instance = get_working_instance()
    if not instance:
        print("CRITICAL: No Piped instances reachable.")
        exit(1)

    for lang_code, query in LANGUAGES.items():
        lessons = scrape_language(instance, lang_code, query)
        
        filename = f"data/lessons_{lang_code}.json"
        
        # Merge with existing if file exists
        if os.path.exists(filename) and len(lessons) > 0:
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    old_data = json.load(f)
                    existing_ids = {l['id'] for l in old_data}
                    for l in lessons:
                        if l['id'] not in existing_ids:
                            old_data.insert(0, l)
                    lessons = old_data[:50] # Keep feed size manageable
            except:
                pass
        
        # Write file (Even if empty, to prevent 404s in app)
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(lessons, f, ensure_ascii=False, indent=2)
            
        print(f"Saved {len(lessons)} videos to {filename}")

if __name__ == "__main__":
    main()